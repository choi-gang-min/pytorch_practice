{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([3, 3])\n",
      "tensor([2., 5., 3.])\n",
      "torch.Size([3])\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.],\n",
      "        [2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "#기본적인 torch array\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [2.,3.,4.]])\n",
    "                    \n",
    "\n",
    "print(t.dim())\n",
    "print(t.size())\n",
    "print(t[:,1])\n",
    "print(t[:,1].size())\n",
    "print((t[:,:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n",
      "tensor([[13., 13.]])\n",
      "tensor([[6., 6.],\n",
      "        [7., 7.]])\n"
     ]
    }
   ],
   "source": [
    "#same sHape\n",
    "\n",
    "m1 = torch.FloatTensor([[3,3]])\n",
    "m2 = torch.FloatTensor([[1,2]])\n",
    "\n",
    "print(m1 + m2)\n",
    "\n",
    "k =  10\n",
    "\n",
    "print(m1 + k)\n",
    "# broad casting\n",
    "m3 = torch.Tensor([[3],\n",
    "                  [4]])\n",
    "\n",
    "print(m1 + m3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 shape:  torch.Size([2, 2])\n",
      "m2 shape:  torch.Size([2, 1])\n",
      "matmul: tensor([[ 5.],\n",
      "        [11.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "##mat mul\n",
    "\n",
    "m1 = torch.Tensor([[1,2],[3,4]])\n",
    "m2 = torch.Tensor([[1],\n",
    "                   [2]])\n",
    "\n",
    "print(\"m1 shape: \", m1.shape)\n",
    "print(\"m2 shape: \", m2.shape)\n",
    "\n",
    "m3 = m1.matmul(m2)\n",
    "print(\"matmul:\" ,m3)\n",
    "\n",
    "\n",
    "\n",
    "## matmul 을 쓰지 않고 * 연산자와 mul 연산자를 쓰는 경우, broadcasting되어 dot product를 수행한다.\n",
    "m3dot= m1*m2\n",
    "print(m3dot)\n",
    "\n",
    "m3mul = m1.mul(m2)\n",
    "\n",
    "print(m3mul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "tensor(1.5000)\n",
      "Axis 0\n",
      "tensor([2.5000, 3.5000, 4.5000])\n",
      "tensor([5., 7., 9.])\n",
      "Axis 1\n",
      "tensor([2., 5.])\n",
      "tensor([ 6., 15.])\n",
      "Axis = -1\n",
      "tensor([2., 5.])\n"
     ]
    }
   ],
   "source": [
    "## 그냥 연산들\n",
    "\n",
    "t = torch.FloatTensor([1,2])\n",
    "print(\"Mean\")\n",
    "print(t.mean())\n",
    "\n",
    "\n",
    "#Axis mean\n",
    "t = torch.FloatTensor([[1,2,3,],\n",
    "                       [4,5,6]])\n",
    "\n",
    "print(\"Axis 0\")\n",
    "print(t.mean(dim= 0))\n",
    "print(t.sum(dim = 0))\n",
    "\n",
    "print(\"Axis 1\")\n",
    "print(t.mean(dim = 1))\n",
    "print(t.sum(dim = 1))\n",
    "\n",
    "\n",
    "print(\"Axis = -1\")\n",
    "print(t.mean(dim = -1))\n",
    "\n",
    "##dim, axis가 외우기 어려우면 다음과 같이 생각하자\n",
    "##0차원부터 시작한다고 하고, 0차원이 그냥 1행 n열이라고 한다면, 이게 기준으로 sum이 된다는건,결국 열로써 더해진다고 생각해라\n",
    "#-1 은 왜 0이랑 똑같지? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis = 0\n",
      "[1.5 2.5 3.5]\n",
      "Axis = 1\n",
      "[2. 3.]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[1,2,3],\n",
    "               [2,3,4]])\n",
    "\n",
    "print(\"Axis = 0\")\n",
    "print(np.mean(a1,axis = 0))\n",
    "\n",
    "print(\"Axis = 1\")\n",
    "print(np.mean(a1,axis = 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [4., 3.]])\n",
      "tensor(4.)\n",
      "tensor(2)\n",
      "MAx: dim = 0  torch.return_types.max(\n",
      "values=tensor([4., 3.]),\n",
      "indices=tensor([1, 1]))\n",
      "argmax axis 0 tensor([1, 1])\n",
      "MAx: dim = 1  torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 0]))\n",
      "argmax axis 1 tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "#max와 argmax\n",
    "\n",
    "t = torch.Tensor([[1,2],[4,3]])\n",
    "\n",
    "print(t)\n",
    "\n",
    "print(t.max())\n",
    "print(t.argmax())\n",
    "\n",
    "\n",
    "print(\"MAx: dim = 0 \", t.max(dim = 0) )\n",
    "print(\"argmax axis 0\",t.argmax(dim = 0))\n",
    "\n",
    "\n",
    "print(\"MAx: dim = 1 \", t.max(dim = 1) )\n",
    "print(\"argmax axis 1\",t.argmax(dim = 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "torch.Size([12])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[[ 1,  2,  3]],\n",
      "\n",
      "        [[ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "#np = reshape, torch = view\n",
    "\n",
    "#먼저, np랑 torch랑 서로 어느정도 연동이 되는듯?\n",
    "\n",
    "a = np.array([[[1, 2, 3],\n",
    "               [4, 5, 6]],\n",
    "              [[7, 8, 9],\n",
    "               [10, 11, 12]]])\n",
    "\n",
    "# ==> tensor로 변환\n",
    "\n",
    "d = torch.tensor(a)\n",
    "\n",
    "print(d.shape)\n",
    "\n",
    "\n",
    "# 여기서 -1 을 넣는것은 앞쪽의 나머지 차원을 하나의 차원으로 묶어버리고, 전부 풀어버리겠다는 거다.\n",
    "print(d.view([-1]))\n",
    "print(d.view([-1]).shape)\n",
    "\n",
    "\n",
    "print(d.view([-1,3]))\n",
    "print(d.view([-1,3]).shape)\n",
    "\n",
    "\n",
    "print(d.view([-1,1,3]))\n",
    "print(d.view([-1,1,3]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "## Tensor manipulation2\n",
    "##1. view // reshaping\n",
    "\n",
    "t = np.array([[[0,1,2],[3,4,5]]],[[[6,7,8],[9,10,11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)\n",
    "\n",
    "# -1은 나머지 차원을 전부 풀어버리겠다는 의미\n",
    "print(ft.view([-1,3])) ## 2 2 3 => 4 3\n",
    "print(ft.view([-1,3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1,1,3]))\n",
    "print(ft.view([-1,1,3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.],\n",
      "        [12.],\n",
      "        [13.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([10., 12., 13.])\n",
      "torch.Size([3])\n",
      "tensor([10., 12., 13.])\n",
      "torch.Size([3])\n",
      "tensor([[10., 12., 13.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# squeeze\n",
    "import torch\n",
    "x = torch.FloatTensor([[10],[12],[13]])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(x.squeeze())\n",
    "print(x.squeeze().shape)\n",
    "\n",
    "# unsqueeze\n",
    "x = torch.FloatTensor([10,12,13])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(x.unsqueeze(0))\n",
    "print(x.unsqueeze(0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# type casting\n",
    "#long, float, double\n",
    "lt = torch.LongTensor([1,2,3,4])\n",
    "print(lt)\n",
    "\n",
    "print(lt.float())\n",
    "\n",
    "bt = torch.ByteTensor([True,False,False,True])\n",
    "print(bt)\n",
    "\n",
    "print(bt.long())\n",
    "print(bt.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#CONCATENATION\n",
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "y = torch.FloatTensor([[5,6],[7,8]])\n",
    "\n",
    "print(torch.cat([x,y],dim = 0))\n",
    "print(torch.cat([x,y],dim = 1))\n",
    "\n",
    "#stacking\n",
    "x = torch.FloatTensor([1,4])\n",
    "y = torch.FloatTensor([2,5])\n",
    "z = torch.FloatTensor([3,6])\n",
    "\n",
    "print(torch.stack([x,y,z]))\n",
    "print(torch.stack([x,y,z],dim = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# stack\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))\n",
    "print(torch.stack([x, y, z], dim=1))\n",
    "\n",
    "\n",
    "# ones_like, zeros_like\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)\n",
    "\n",
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place operation\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(x.mul(2.))\n",
    "print(x)\n",
    "print(x.mul_(2.))\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1663], requires_grad=True) tensor([0.4922], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "import torch\n",
    "\n",
    "x = torch.FloatTensor([[1], [2], [3]])\n",
    "y = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "w = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "#torch 에서 간단하게 SGD 사용하기.\n",
    "\n",
    "optimizer = torch.optim.SGD([w,b], lr = 0.01) # optimizer SGD or Adam or RMSprop 중 선택하기.\n",
    "for i in range(10):\n",
    "    hypothesis = x*w + b\n",
    "    cost = torch.mean((hypothesis - y) ** 2)\n",
    "    optimizer.zero_grad() # 이거 안하면, 기존의 gradient에 더해져서, 값이 더욱 누적되어버린다. 즉, 이전의 gradient가 새로운 계산에 영향을 끼친다.\n",
    "    cost.backward() # 이거 하면, w.grad, b.grad에 gradient가 저장된다.\n",
    "    # print(optimizer.zero_grad())\n",
    "    optimizer.step() # 이거 하면, w, b에 gradient가 적용된다.\n",
    "    # print(optimizer.step())\n",
    "print(w,b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
